{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShristiJoshi/College-website/blob/main/Newcode_Gorkhapatra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsuXLhnVt7ZK"
      },
      "outputs": [],
      "source": [
        "!pip install requests>=2.31.0 \\\n",
        "    beautifulsoup4>=4.12.0 \\\n",
        "    pandas>=2.0.0 \\\n",
        "    openpyxl>=3.1.0 \\\n",
        "    lxml>=4.9.0 \\\n",
        "    html5lib>=1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-XUAf_cuN_s",
        "outputId": "0d050c14-dd37-468f-d78a-41eb18e0ded8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Enhanced Gorkhapatra News Scraper...\n",
            "============================================================\n",
            "Enter maximum number of articles to scrape (default 20): 20\n",
            "\n",
            "üì∞ Scraping up to 20 articles from Gorkhapatra Online...\n",
            "‚è≥ This may take a few minutes...\n",
            "\n",
            "\n",
            "============================================================\n",
            "‚úÖ SCRAPING COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "üìä Total Articles Scraped: 10\n",
            "‚è±Ô∏è  Time Taken: 42.79 seconds\n",
            "üìÅ Data Saved To: enhanced_gorkhapatra_news_20251204_062842.xlsx\n",
            "\n",
            "üìà Category Distribution:\n",
            "   N/A: 10\n",
            "\n",
            "üë• Author Distribution:\n",
            "   N/A: 10\n",
            "\n",
            "üì∞ Sample Articles:\n",
            "\n",
            "Article 1:\n",
            "  üìù Title: ‡§µ‡§ø‡§∑‡§æ‡§¶‡•Ä ‡§®‡§ø‡§Ø‡§®‡•ç‡§§‡•ç‡§∞‡§£‡§ï‡•ã ‡§Ü‡§¶‡•á‡§∂...\n",
            "  üìÖ Date: N/A\n",
            "  üè∑Ô∏è  Category: N/A\n",
            "  ‚úçÔ∏è  Author: N/A\n",
            "  üìÑ Content: ‡§§‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§§‡§•‡§æ ‡§´‡§≤‡§´‡•Ç‡§≤‡§Æ‡§æ ‡§π‡§æ‡§≤‡§ø‡§è‡§ï‡•ã ‡§µ‡§ø‡§∑ ‡§Æ‡§æ‡§®‡§ø‡§∏‡§ï‡•ã ‡§∂‡§∞‡•Ä‡§∞‡§Æ‡§æ ‡§Æ‡§®‡•ç‡§¶ ‡§Ö‡§∏‡§∞ ‡§ó‡§∞‡•ç‡§õ ‡•§ ‡§Ø‡§∏‡•ç‡§§‡•ã ‡§µ‡§ø‡§∑‡§≤‡•á ‡§§‡§§‡•ç‡§ï‡§æ‡§≤ ‡§Ö‡§∏‡§∞ ‡§®‡§ó‡§∞‡•á ‡§™‡§®‡§ø ‡§¨‡§ø‡§∏‡•ç‡§§‡§æ...\n",
            "\n",
            "Article 2:\n",
            "  üìù Title: ‡§Æ‡•É‡§§ ‡§Ö‡§µ‡§∏‡•ç‡§•‡§æ‡§Æ‡§æ ‡§≠‡•á‡§ü‡§ø‡§è ‡§™‡•ç‡§∞‡§ï‡§æ‡§∂...\n",
            "  üìÖ Date: N/A\n",
            "  üè∑Ô∏è  Category: N/A\n",
            "  ‚úçÔ∏è  Author: N/A\n",
            "  üìÑ Content: ‡§Æ‡•É‡§§‡§ï ‡§≠‡•á‡§≤‡§æ ‡§™‡§∞‡•ç‡§®‡•Å‡§≠‡§è‡§ï‡•ã ‡§™‡•ç‡§∞‡§ï‡§æ‡§∂ ‡§§‡§æ‡§Æ‡§æ‡§ô ‡•§ ‡§§‡§∏‡•ç‡§¨‡§ø‡§∞ : ‡§∞‡§æ‡§ß‡§æ ‡§≤‡•Å‡§á‡§ü‡•á‡§≤ ‡§∞‡§æ‡§ß‡§æ ‡§≤‡•Å‡§á‡§ü‡•á‡§≤‡§´‡§ø‡§¶‡§ø‡§Æ, ‡§Æ‡§ô‡•ç‡§∏‡§ø‡§∞ ‡•ß‡•Æ ‡§ó‡§§‡•á ‡•§‡§™‡§æ‡§Å‡§ö‡§•‡§∞‡§ï‡•ã ‡§´‡§ø...\n",
            "\n",
            "Article 3:\n",
            "  üìù Title: ‡§®‡•á‡§™‡§æ‡§≤‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∂‡•ç‡§∞‡•Ä‡§≤‡§ô‡•ç‡§ï‡§æ‡§Æ‡§æ ‡§¨‡§æ‡§¢‡•Ä‡§¨‡§æ‡§ü ‡§≠‡§è‡§ï‡•ã ‡§ï‡•ç‡§∑‡§§‡§ø‡§™‡•ç‡§∞‡§§‡§ø ‡§¶‡•Å‡§É‡§ñ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§...\n",
            "  üìÖ Date: N/A\n",
            "  üè∑Ô∏è  Category: N/A\n",
            "  ‚úçÔ∏è  Author: N/A\n",
            "  üìÑ Content: ‡§ï‡§æ‡§†‡§Æ‡§æ‡§°‡•å‡§Å, ‡§Æ‡§ô‡•ç‡§∏‡§ø‡§∞ ‡•ß‡•´¬† ‡§ó‡§§‡•á ‡•§ ‡§®‡•á‡§™‡§æ‡§≤ ‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á ‡§∂‡•ç‡§∞‡•Ä‡§≤‡§ô‡•ç‡§ï‡§æ‡§Æ‡§æ ‡§π‡§æ‡§≤‡•à ‡§Ü‡§è‡§ï‡•ã ‡§µ‡§ø‡§®‡§æ‡§∂‡§ï‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§¢‡•Ä‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§Ö‡§®‡•á‡§ï‡•å‡§Å ‡§ú‡§®‡§ú‡•Ä‡§µ‡§®‡§ï‡•ã...\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import re\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class EnhancedGorkhapatraScraper:\n",
        "    def __init__(self):\n",
        "        self.base_url = \"https://gorkhapatraonline.com/\"\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        }\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(self.headers)\n",
        "        self.categories = {\n",
        "            '‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø': 'national',\n",
        "            '‡§≤‡•ã‡§ï‡§∏‡•á‡§µ‡§æ': 'public-service',\n",
        "            '‡§∞‡§æ‡§ú‡§®‡•Ä‡§§‡§ø': 'politics',\n",
        "            '‡§Ö‡§∞‡•ç‡§•': 'economy',\n",
        "            '‡§µ‡§ø‡§ö‡§æ‡§∞': 'opinion',\n",
        "            '‡§ñ‡•á‡§≤‡§ï‡•Å‡§¶': 'sports',\n",
        "            '‡§Æ‡§®‡•ã‡§∞‡§û‡•ç‡§ú‡§®': 'entertainment',\n",
        "            '‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø': 'health',\n",
        "            '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ': 'education',\n",
        "            '‡§™‡•ç‡§∞‡§µ‡§ø‡§ß‡§ø': 'technology',\n",
        "            '‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§ø‡§Ø': 'international'\n",
        "        }\n",
        "\n",
        "    def get_page_content(self, url, retries=3):\n",
        "        \"\"\"Fetch page content with retry mechanism and better error handling\"\"\"\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=20)\n",
        "                response.raise_for_status()\n",
        "                response.encoding = 'utf-8'\n",
        "                logger.info(f\"Successfully fetched: {url}\")\n",
        "                return response.text\n",
        "            except requests.RequestException as e:\n",
        "                logger.warning(f\"Attempt {attempt + 1} failed for {url}: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                else:\n",
        "                    logger.error(f\"Failed to fetch {url} after {retries} attempts\")\n",
        "                    return None\n",
        "        return None\n",
        "\n",
        "    def extract_article_links(self, html_content):\n",
        "        \"\"\"Extract article links using Gorkhapatra-specific selectors\"\"\"\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        article_links = []\n",
        "\n",
        "        # Gorkhapatra-specific selectors based on website structure\n",
        "        selectors = [\n",
        "            # Main news headlines\n",
        "            'h2 a', 'h3 a', 'h4 a',\n",
        "\n",
        "            # News section links\n",
        "            '.news-section a',\n",
        "            '.article-section a',\n",
        "            '.main-content a',\n",
        "\n",
        "            # Specific to Gorkhapatra structure\n",
        "            'a[href*=\"/news/\"]',\n",
        "            'a[href*=\"/article/\"]',\n",
        "            'a[href*=\"/story/\"]',\n",
        "\n",
        "            # Look for links in main content areas\n",
        "            'main a',\n",
        "            '.content-area a',\n",
        "            '.news-area a'\n",
        "        ]\n",
        "\n",
        "        for selector in selectors:\n",
        "            try:\n",
        "                links = soup.select(selector)\n",
        "                for link in links:\n",
        "                    href = link.get('href')\n",
        "                    if href and self.is_valid_article_url(href):\n",
        "                        full_url = self.build_full_url(href)\n",
        "                        if full_url and full_url not in article_links:\n",
        "                            article_links.append(full_url)\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"Selector {selector} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Remove duplicates and limit\n",
        "        article_links = list(set(article_links))[:10]\n",
        "        logger.info(f\"Found {len(article_links)} potential article links\")\n",
        "\n",
        "        return article_links\n",
        "\n",
        "    def is_valid_article_url(self, url):\n",
        "        \"\"\"Check if URL is a valid article URL for Gorkhapatra\"\"\"\n",
        "        exclude_patterns = [\n",
        "            '/category/', '/tag/', '/author/', '/page/', '/search',\n",
        "            '/about', '/contact', '/privacy', '/terms', '/advertise',\n",
        "            '/subscribe', '/login', '/register', '/admin', '/wp-admin',\n",
        "            '/feed', '.pdf', '.jpg', '.png', '.gif', '.css', '.js',\n",
        "            '#', 'javascript:', 'mailto:', 'tel:'\n",
        "        ]\n",
        "\n",
        "        url_lower = url.lower()\n",
        "        for pattern in exclude_patterns:\n",
        "            if pattern in url_lower:\n",
        "                return False\n",
        "\n",
        "        # Must be a relative URL or same domain\n",
        "        if url.startswith('http') and 'gorkhapatraonline.com' not in url:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def build_full_url(self, href):\n",
        "        \"\"\"Convert relative URLs to absolute URLs\"\"\"\n",
        "        if href.startswith('http'):\n",
        "            return href\n",
        "        elif href.startswith('/'):\n",
        "            return self.base_url.rstrip('/') + href\n",
        "        else:\n",
        "            return self.base_url + href\n",
        "\n",
        "    def extract_article_data(self, article_url):\n",
        "        \"\"\"Extract comprehensive article data with Gorkhapatra-specific selectors\"\"\"\n",
        "        html_content = self.get_page_content(article_url)\n",
        "        if not html_content:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        try:\n",
        "            # Extract title\n",
        "            title = self.extract_title(soup)\n",
        "\n",
        "            # Extract publication date\n",
        "            publication_date = self.extract_publication_date(soup)\n",
        "\n",
        "            # Extract body content\n",
        "            body_content = self.extract_body_content(soup)\n",
        "\n",
        "            # Extract category\n",
        "            category = self.extract_category(soup)\n",
        "\n",
        "            # Extract author\n",
        "            author = self.extract_author(soup)\n",
        "\n",
        "            # Extract summary/excerpt\n",
        "            summary = self.extract_summary(soup)\n",
        "\n",
        "            # Extract tags\n",
        "            tags = self.extract_tags(soup)\n",
        "\n",
        "            # Extract image URLs\n",
        "            images = self.extract_images(soup)\n",
        "\n",
        "            # Validate data\n",
        "            if title and body_content and len(body_content) > 50:\n",
        "                return {\n",
        "                    'Title': title,\n",
        "                    'Publication_Date': publication_date,\n",
        "                    'Author': author,\n",
        "                    'Category': category,\n",
        "                    'Summary': summary,\n",
        "                    'Body_Content': body_content,\n",
        "                    'Tags': tags,\n",
        "                    'Images': images,\n",
        "                    'Article_URL': article_url,\n",
        "                    'Scraped_At': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(f\"Insufficient data extracted from {article_url}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting data from {article_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_title(self, soup):\n",
        "        \"\"\"Extract article title with multiple fallback selectors\"\"\"\n",
        "        title_selectors = [\n",
        "            'h1.article-title',\n",
        "            'h1.news-title',\n",
        "            'h1',\n",
        "            '.article-title h1',\n",
        "            '.news-title h1',\n",
        "            '.headline h1',\n",
        "            'title'\n",
        "        ]\n",
        "\n",
        "        for selector in title_selectors:\n",
        "            elem = soup.select_one(selector)\n",
        "            if elem:\n",
        "                title = elem.get_text(strip=True)\n",
        "                if title and len(title) > 5:\n",
        "                    return title\n",
        "        return \"N/A\"\n",
        "\n",
        "    def extract_publication_date(self, soup):\n",
        "        \"\"\"Extract publication date with multiple formats\"\"\"\n",
        "        date_selectors = [\n",
        "            '.published-date',\n",
        "            '.article-date',\n",
        "            '.news-date',\n",
        "            '.meta-date',\n",
        "            '.timestamp',\n",
        "            'time',\n",
        "            '.date'\n",
        "        ]\n",
        "\n",
        "        for selector in date_selectors:\n",
        "            elem = soup.select_one(selector)\n",
        "            if elem:\n",
        "                text = elem.get_text(strip=True)\n",
        "                # Try to extract date from text\n",
        "                date = self.parse_date_text(text)\n",
        "                if date:\n",
        "                    return date\n",
        "                return text\n",
        "\n",
        "        return \"N/A\"\n",
        "\n",
        "    def parse_date_text(self, text):\n",
        "        \"\"\"Parse various date formats\"\"\"\n",
        "        # Nepali date patterns\n",
        "        nepali_patterns = [\n",
        "            r'(\\d{1,2}\\s+[‡§≠‡§¶‡•å|‡§Æ‡§Ç‡§∏‡§ø‡§∞|‡§™‡•Å‡§∑|‡§Æ‡§æ‡§ò|‡§´‡§æ‡§≤‡•ç‡§ó‡•Å‡§®|‡§ö‡•à‡§§|‡§¨‡•à‡§∂‡§æ‡§ñ|‡§ú‡•á‡§†|‡§Ö‡§∏‡§æ‡§∞|‡§∂‡•ç‡§∞‡§æ‡§µ‡§£|‡§≠‡§¶‡•å|‡§Ö‡§∏‡•ã‡§ú|‡§ï‡§æ‡§∞‡•ç‡§§‡§ø‡§ï|‡§Æ‡§Ç‡§∏‡§ø‡§∞]+)\\s+(\\d{4})',\n",
        "            r'(\\d{1,2}\\s+[‡§≠‡§¶‡•å|‡§Æ‡§Ç‡§∏‡§ø‡§∞|‡§™‡•Å‡§∑|‡§Æ‡§æ‡§ò|‡§´‡§æ‡§≤‡•ç‡§ó‡•Å‡§®|‡§ö‡•à‡§§|‡§¨‡•à‡§∂‡§æ‡§ñ|‡§ú‡•á‡§†|‡§Ö‡§∏‡§æ‡§∞|‡§∂‡•ç‡§∞‡§æ‡§µ‡§£|‡§≠‡§¶‡•å|‡§Ö‡§∏‡•ã‡§ú|‡§ï‡§æ‡§∞‡•ç‡§§‡§ø‡§ï|‡§Æ‡§Ç‡§∏‡§ø‡§∞]+)',\n",
        "        ]\n",
        "\n",
        "        # English date patterns\n",
        "        english_patterns = [\n",
        "            r'(\\d{4}[-/]\\d{1,2}[-/]\\d{1,2})',\n",
        "            r'(\\d{1,2}[-/]\\d{1,2}[-/]\\d{4})',\n",
        "            r'(\\w+\\s+\\d{1,2},?\\s+\\d{4})',\n",
        "            r'(\\d{1,2}\\s+\\w+\\s+\\d{4})'\n",
        "        ]\n",
        "\n",
        "        for pattern in nepali_patterns + english_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(0)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_body_content(self, soup):\n",
        "        \"\"\"Extract article body content\"\"\"\n",
        "        content_selectors = [\n",
        "            '.article-body',\n",
        "            '.news-body',\n",
        "            '.story-body',\n",
        "            '.post-body',\n",
        "            '.entry-content',\n",
        "            '.content',\n",
        "            '.article-content',\n",
        "            '.story-content',\n",
        "            'article .content',\n",
        "            '.main-content'\n",
        "        ]\n",
        "\n",
        "        for selector in content_selectors:\n",
        "            container = soup.select_one(selector)\n",
        "            if container:\n",
        "                paragraphs = container.find_all('p')\n",
        "                if paragraphs:\n",
        "                    content_list = []\n",
        "                    for p in paragraphs:\n",
        "                        text = p.get_text(strip=True)\n",
        "                        if text and len(text) > 20:\n",
        "                            content_list.append(text)\n",
        "\n",
        "                    if content_list:\n",
        "                        return ' '.join(content_list)\n",
        "\n",
        "        # Fallback: try to get all paragraphs\n",
        "        all_paragraphs = soup.find_all('p')\n",
        "        content_list = []\n",
        "        for p in all_paragraphs:\n",
        "            text = p.get_text(strip=True)\n",
        "            if text and len(text) > 30:\n",
        "                content_list.append(text)\n",
        "\n",
        "        return ' '.join(content_list) if content_list else \"N/A\"\n",
        "\n",
        "    def extract_category(self, soup):\n",
        "        \"\"\"Extract article category\"\"\"\n",
        "        category_selectors = [\n",
        "            '.category',\n",
        "            '.article-category',\n",
        "            '.news-category',\n",
        "            '.breadcrumb a',\n",
        "            '.breadcrumbs a',\n",
        "            '.meta-category',\n",
        "            '.tag'\n",
        "        ]\n",
        "\n",
        "        for selector in category_selectors:\n",
        "            elem = soup.select_one(selector)\n",
        "            if elem:\n",
        "                category = elem.get_text(strip=True)\n",
        "                if category and len(category) > 2:\n",
        "                    return category\n",
        "\n",
        "        return \"N/A\"\n",
        "\n",
        "    def extract_author(self, soup):\n",
        "        \"\"\"Extract article author\"\"\"\n",
        "        author_selectors = [\n",
        "            '.author',\n",
        "            '.article-author',\n",
        "            '.news-author',\n",
        "            '.byline',\n",
        "            '.meta-author',\n",
        "            '.writer'\n",
        "        ]\n",
        "\n",
        "        for selector in author_selectors:\n",
        "            elem = soup.select_one(selector)\n",
        "            if elem:\n",
        "                author = elem.get_text(strip=True)\n",
        "                if author and len(author) > 2:\n",
        "                    return author\n",
        "\n",
        "        return \"N/A\"\n",
        "\n",
        "    def extract_summary(self, soup):\n",
        "        \"\"\"Extract article summary/excerpt\"\"\"\n",
        "        summary_selectors = [\n",
        "            '.summary',\n",
        "            '.excerpt',\n",
        "            '.article-summary',\n",
        "            '.news-summary',\n",
        "            '.description',\n",
        "            '.meta-description'\n",
        "        ]\n",
        "\n",
        "        for selector in summary_selectors:\n",
        "            elem = soup.select_one(selector)\n",
        "            if elem:\n",
        "                summary = elem.get_text(strip=True)\n",
        "                if summary and len(summary) > 20:\n",
        "                    return summary\n",
        "\n",
        "        return \"N/A\"\n",
        "\n",
        "    def extract_tags(self, soup):\n",
        "        \"\"\"Extract article tags\"\"\"\n",
        "        tag_selectors = [\n",
        "            '.tags a',\n",
        "            '.tag a',\n",
        "            '.article-tags a',\n",
        "            '.news-tags a',\n",
        "            '.meta-tags a'\n",
        "        ]\n",
        "\n",
        "        tags = []\n",
        "        for selector in tag_selectors:\n",
        "            elements = soup.select(selector)\n",
        "            for elem in elements:\n",
        "                tag = elem.get_text(strip=True)\n",
        "                if tag and len(tag) > 2:\n",
        "                    tags.append(tag)\n",
        "\n",
        "        return ', '.join(tags) if tags else \"N/A\"\n",
        "\n",
        "    def extract_images(self, soup):\n",
        "        \"\"\"Extract article images\"\"\"\n",
        "        image_selectors = [\n",
        "            '.article-image img',\n",
        "            '.news-image img',\n",
        "            '.story-image img',\n",
        "            '.post-image img',\n",
        "            '.content img',\n",
        "            'article img'\n",
        "        ]\n",
        "\n",
        "        images = []\n",
        "        for selector in image_selectors:\n",
        "            elements = soup.select(selector)\n",
        "            for elem in elements:\n",
        "                src = elem.get('src')\n",
        "                if src:\n",
        "                    if src.startswith('/'):\n",
        "                        src = self.base_url.rstrip('/') + src\n",
        "                    elif not src.startswith('http'):\n",
        "                        src = self.base_url + src\n",
        "                    images.append(src)\n",
        "\n",
        "        return ', '.join(images) if images else \"N/A\"\n",
        "\n",
        "    def scrape_articles(self, max_articles=20):\n",
        "        \"\"\"Scrape articles with enhanced error handling and progress tracking\"\"\"\n",
        "        logger.info(f\"Starting enhanced scraping of {self.base_url}\")\n",
        "\n",
        "        main_page = self.get_page_content(self.base_url)\n",
        "        if not main_page:\n",
        "            logger.error(\"Failed to fetch main page\")\n",
        "            return []\n",
        "\n",
        "        article_links = self.extract_article_links(main_page)\n",
        "        if not article_links:\n",
        "            logger.warning(\"No article links found\")\n",
        "            return []\n",
        "\n",
        "        articles_data = []\n",
        "        successful_scrapes = 0\n",
        "        failed_scrapes = 0\n",
        "\n",
        "        logger.info(f\"Starting to scrape {len(article_links)} articles...\")\n",
        "\n",
        "        for i, link in enumerate(article_links, 1):\n",
        "            if len(articles_data) >= max_articles:\n",
        "                break\n",
        "\n",
        "            logger.info(f\"Scraping article {i}/{len(article_links)}: {link}\")\n",
        "\n",
        "            try:\n",
        "                data = self.extract_article_data(link)\n",
        "                if data:\n",
        "                    articles_data.append(data)\n",
        "                    successful_scrapes += 1\n",
        "                    logger.info(f\"‚úÖ Successfully scraped: {data['Title'][:60]}...\")\n",
        "                else:\n",
        "                    failed_scrapes += 1\n",
        "                    logger.warning(f\"‚ùå Failed to extract data from: {link}\")\n",
        "            except Exception as e:\n",
        "                failed_scrapes += 1\n",
        "                logger.error(f\"‚ùå Error scraping {link}: {e}\")\n",
        "\n",
        "            # Polite delay between requests\n",
        "            time.sleep(2)\n",
        "\n",
        "        logger.info(f\"Scraping completed: {successful_scrapes} successful, {failed_scrapes} failed\")\n",
        "        return articles_data\n",
        "\n",
        "    def save_to_excel(self, articles_data, filename=None):\n",
        "        \"\"\"Save scraped data to Excel with enhanced formatting\"\"\"\n",
        "        if not articles_data:\n",
        "            logger.warning(\"No data to save\")\n",
        "            return None\n",
        "\n",
        "        if not filename:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"enhanced_gorkhapatra_news_{timestamp}.xlsx\"\n",
        "\n",
        "        try:\n",
        "            df = pd.DataFrame(articles_data)\n",
        "\n",
        "            # Reorder columns for better readability\n",
        "            column_order = [\n",
        "                'Title', 'Publication_Date', 'Author', 'Category',\n",
        "                'Summary', 'Body_Content', 'Tags', 'Images',\n",
        "                'Article_URL', 'Scraped_At'\n",
        "            ]\n",
        "\n",
        "            # Only include columns that exist in the data\n",
        "            existing_columns = [col for col in column_order if col in df.columns]\n",
        "            df = df[existing_columns]\n",
        "\n",
        "            # Save to Excel with better formatting\n",
        "            with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "                df.to_excel(writer, sheet_name='News_Data', index=False)\n",
        "\n",
        "                # Auto-adjust column widths\n",
        "                worksheet = writer.sheets['News_Data']\n",
        "                for column in worksheet.columns:\n",
        "                    max_length = 0\n",
        "                    column_letter = column[0].column_letter\n",
        "                    for cell in column:\n",
        "                        try:\n",
        "                            if len(str(cell.value)) > max_length:\n",
        "                                max_length = len(str(cell.value))\n",
        "                        except:\n",
        "                            pass\n",
        "                    adjusted_width = min(max_length + 2, 100)\n",
        "                    worksheet.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "            logger.info(f\"Data saved to {filename}\")\n",
        "            return filename\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving to Excel: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_scraping_stats(self, articles_data):\n",
        "        \"\"\"Get statistics about scraped data\"\"\"\n",
        "        if not articles_data:\n",
        "            return {}\n",
        "\n",
        "        stats = {\n",
        "            'total_articles': len(articles_data),\n",
        "            'categories': {},\n",
        "            'authors': {},\n",
        "            'date_range': {'earliest': None, 'latest': None}\n",
        "        }\n",
        "\n",
        "        for article in articles_data:\n",
        "            # Count categories\n",
        "            category = article.get('Category', 'Unknown')\n",
        "            stats['categories'][category] = stats['categories'].get(category, 0) + 1\n",
        "\n",
        "            # Count authors\n",
        "            author = article.get('Author', 'Unknown')\n",
        "            stats['authors'][author] = stats['authors'].get(author, 0) + 1\n",
        "\n",
        "        return stats\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with enhanced error handling and user feedback\"\"\"\n",
        "    try:\n",
        "        print(\"üöÄ Starting Enhanced Gorkhapatra News Scraper...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        scraper = EnhancedGorkhapatraScraper()\n",
        "\n",
        "        # Get user preference for number of articles\n",
        "        try:\n",
        "            max_articles = int(input(\"Enter maximum number of articles to scrape (default 20): \") or \"20\")\n",
        "        except ValueError:\n",
        "            max_articles = 20\n",
        "            print(\"Invalid input, using default: 20 articles\")\n",
        "\n",
        "        print(f\"\\nüì∞ Scraping up to {max_articles} articles from Gorkhapatra Online...\")\n",
        "        print(\"‚è≥ This may take a few minutes...\\n\")\n",
        "\n",
        "        # Start scraping\n",
        "        start_time = time.time()\n",
        "        articles = scraper.scrape_articles(max_articles=max_articles)\n",
        "        end_time = time.time()\n",
        "\n",
        "        if articles:\n",
        "            # Save data\n",
        "            filename = scraper.save_to_excel(articles)\n",
        "\n",
        "            # Get statistics\n",
        "            stats = scraper.get_scraping_stats(articles)\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"‚úÖ SCRAPING COMPLETED SUCCESSFULLY!\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"üìä Total Articles Scraped: {len(articles)}\")\n",
        "            print(f\"‚è±Ô∏è  Time Taken: {end_time - start_time:.2f} seconds\")\n",
        "            print(f\"üìÅ Data Saved To: {filename}\")\n",
        "\n",
        "            print(f\"\\nüìà Category Distribution:\")\n",
        "            for category, count in stats['categories'].items():\n",
        "                print(f\"   {category}: {count}\")\n",
        "\n",
        "            print(f\"\\nüë• Author Distribution:\")\n",
        "            for author, count in stats['authors'].items():\n",
        "                if count > 1:  # Only show authors with multiple articles\n",
        "                    print(f\"   {author}: {count}\")\n",
        "\n",
        "            print(f\"\\nüì∞ Sample Articles:\")\n",
        "            for i, article in enumerate(articles[:3], 1):\n",
        "                print(f\"\\nArticle {i}:\")\n",
        "                print(f\"  üìù Title: {article['Title'][:80]}...\")\n",
        "                print(f\"  üìÖ Date: {article['Publication_Date']}\")\n",
        "                print(f\"  üè∑Ô∏è  Category: {article['Category']}\")\n",
        "                print(f\"  ‚úçÔ∏è  Author: {article['Author']}\")\n",
        "                print(f\"  üìÑ Content: {article['Body_Content'][:100]}...\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\n‚ùå No articles were scraped successfully\")\n",
        "            print(\"üí° Possible issues:\")\n",
        "            print(\"   - Website structure may have changed\")\n",
        "            print(\"   - Network connection problems\")\n",
        "            print(\"   - Website blocking automated requests\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n‚ö†Ô∏è Scraping interrupted by user\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {e}\")\n",
        "        print(f\"\\n‚ùå An unexpected error occurred: {e}\")\n",
        "        print(\"üí° Check the logs for more details\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJKXMv4SuXzF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "6b3833a3-5e21-4393-99ed-fadbdb18e8ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: enhanced_gorkhapatra_news_20250829_170333.xlsx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3870179865.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"enhanced_gorkhapatra_news_20250829_170333.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: enhanced_gorkhapatra_news_20250829_170333.xlsx"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"enhanced_gorkhapatra_news_20250829_170333.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwg86RRCxsbR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VqYuMXI82W0I-diBRVgCxK2saI_B4Y4F",
      "authorship_tag": "ABX9TyOO5mMpar06lbmm1CbAEYCN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}